# GPT-2 Customization

Developing a customized small-scale Chinese GPT-2 model for use on a personal computer.

## Current Status
### Completed Tasks
- **Project Initialization**: Successfully created the local Git repository and set up the basic project structure.
- **Environment Setup**: Completed the basic environment configuration, including Python and necessary dependencies installation.
- **Model Preparation**: Collecting and preprocessing the data needed for training a small-scale Chinese GPT-2 model.

### Ongoing Tasks
- **Researching and Implementing Custom Tokenizer**: Investigating methods to create a custom tokenizer table optimized for Chinese text, and developing a tokenizer that leverages this table to preprocess and tokenize the Chinese text data.

## Development Plan
- Researching tokenization methods to optimize the model size.
- Using Few-Shot Learning to train the GPT-2 model to capture the dialogue structure in novels, and using this for subsequent labeling tasks.
- Fine-tuning the model using dialogue from novels as training data.
- Once the GPT-2 model is trained, observing the interaction between two simulated characters using two GPT-2 models.
