# GPT-2 Customization

Developing a customized small-scale Chinese GPT-2 model for use on a personal computer.

## Current Status
- **Project Initialization**: Successfully created the local Git repository and set up the basic project structure.
- **Environment Setup**: Completed the basic environment configuration, including Python and necessary dependencies installation.
- **Model Preparation**: Collecting and preprocessing the data needed for training a small-scale Chinese GPT-2 model.

## Development Plan
1. Currently researching tokenization methods to optimize the model size.
2. Using Few-Shot Learning to train the GPT-2 model to capture the dialogue structure in novels, and using this for subsequent labeling tasks.
3. Fine-tuning the model using dialogue from novels as training data.
4. Once the GPT-2 model is trained, observing the interaction between two simulated characters using two GPT-2 models.
